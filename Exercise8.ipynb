{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9a5da7",
   "metadata": {},
   "source": [
    "# Exercise 8: Seeing the Brain “Learn” through Reward Prediction Errors\n",
    "\n",
    "In **[BuildingABrain.ipynb](https://colab.research.google.com/github/NVDLI/notebooks/blob/master/building-a-brain/BuildingABrain.ipynb)**, you watched a tiny neural network learn to classify Fashion-MNIST images by **trial and error**: it made guesses, compared them to the true label (flashcard answer), computed a **prediction error**, and updated its weights to reduce that error. Over many epochs, the network’s accuracy climbed as its internal connections strengthened in exactly the same way our brains adapt during reward-based learning.\n",
    "\n",
    "---\n",
    "\n",
    "## A quick look at the MID task\n",
    "\n",
    "Here we move from artificial networks back to human brains, using the **Monetary Incentive Delay (MID)** task. In the MID:\n",
    "\n",
    "1. Participants see a cue predicting potential reward, neutral outcome, or loss.  \n",
    "2. They respond quickly to a target to try to earn money (or avoid losing it).  \n",
    "3. Feedback tells them if they succeeded, generating a **prediction error** when outcome differs from expectation.  \n",
    "\n",
    "> **See “Guide to Exercise 8” slides** for a step-by-step demo of the MID timing, trial types, and expected brain responses.\n",
    "\n",
    "---\n",
    "\n",
    "## What you will do in this notebook (and why)\n",
    "\n",
    "### 1. **QC Anatomy**  \n",
    "You will load brain images and verify the alignment of your region of interest (ROI)—the nucleus accumbens (NAcc).  \n",
    "**Why?**  \n",
    "✅ To make sure the mask correctly covers the NAcc (more on the mask below). If the mask is off, any measurements from it would not represent true NAcc activity.\n",
    "\n",
    "#### Exploring the NAcc in the ABCD Data Dictionary\n",
    "\n",
    "For a deeper understanding of the nucleus accumbens (NAcc), look up the following metrics in the ABCD data dictionary and visualize them using the Brain Atlas Visualizer:\n",
    "https://abcd.deapscience.com/#/my-datasets/create-dataset\n",
    "\n",
    "1. **mr_y_smri__vol__aseg__ab__lh_sum (and _rh_sum)**\n",
    "   - **What it measures:** Total volume (mm³) of the left/right nucleus accumbens from FreeSurfer’s subcortical segmentation.\n",
    "   - **Why it’s useful:** This is the classic structural metric—literally the size of the NAcc. It’s stable, easy to interpret, and visually clear when you explore the region in the ABCD Brain Atlas Visualizer.\n",
    "\n",
    "2. **mr_y_smri__t1__aseg__ab__lh_mean (and _rh_mean)**\n",
    "   - **What it measures:** Mean T1-weighted intensity within the NAcc (left/right).\n",
    "   - **Why it’s useful:** This reflects tissue contrast and integrity. It isn’t a “size” measure but a signal measure related to microstructural properties (e.g., myelination, iron content, water density). It provides a complementary way of looking at NAcc structure beyond just volume.\n",
    "---\n",
    "\n",
    "### 2. **Extract NAcc Time-Series**  \n",
    "You will extract the BOLD signal (functional MRI activation) from the NAcc across time. “Blood-Oxygen-Level-Dependent (BOLD) signal”: this a measure of blood flow to a brain region, which we use as a proxy for neural activity. \n",
    "\n",
    "**Why?**  \n",
    "✅ To isolate the activity from our key brain region, so we can later connect its behavior to reward anticipation and feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Build Reward vs Neutral Arrays**  \n",
    "You will separate the NAcc activation values into two groups: Reward trials vs Neutral trials.  \n",
    "**Why?**  \n",
    "✅ To directly compare how strongly the NAcc responds to rewarding cues versus neutral cues, helping test the incentive-salience hypothesis.  \n",
    "🧠 The **incentive-salience hypothesis** proposes that dopamine transforms neutral cues into “wanted” signals, giving them motivational power to drive behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Visualizations & Simple Probabilistic Insights**  \n",
    "You will generate several figures:  \n",
    "- Box plot of means  \n",
    "- Violin plot of full distributions  \n",
    "- Empirical probability $P(\\mathrm{Reward} > \\mathrm{Neutral})$  \n",
    "- Bootstrap confidence interval  \n",
    "- Whole-brain activation maps  \n",
    "\n",
    "**Why?**  \n",
    "✅ Different plots let you see the data from multiple perspectives: central tendency, spread, uncertainty, and brain location.  \n",
    "✅ Combining visual and statistical approaches builds a stronger, more trustworthy analysis.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s get started! 🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Import Libraries & Configure Environment\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# fMRI analysis\n",
    "from nilearn import image, masking\n",
    "from nilearn.image import resample_to_img\n",
    "from nilearn.masking import apply_mask\n",
    "\n",
    "# Interactive widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Network utilities (for data download)\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "\n",
    "# Create output folder for figures\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "\n",
    "print(\"✅ Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff413962",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Source, Citation & Folder Layout\n",
    "\n",
    "**🚨 TEAMWORK NOTE (read carefully!)**  \n",
    "You can work with any of the four available subjects:\n",
    "- **Subject 01:** set `SUBJECT = \"sub-s001\"`  \n",
    "- **Subject 02:** set `SUBJECT = \"sub-s002\"`  \n",
    "- **Subject 03:** set `SUBJECT = \"sub-s003\"`  \n",
    "- **Subject 04:** set `SUBJECT = \"sub-s004\"`  \n",
    "\n",
    "In this exercise we use **real fMRI data** from the Adolescent Health and Development in Context (AHDC) study, publicly available on OpenNeuro:\n",
    "\n",
    "> Baldwin M. Way, Christopher R. Browning, Dylan D. Wagner, Jodi L. Ford, Bethany Boettner & Ping Bai (2025).  \n",
    "> _Structural and functional MRI dataset from the Adolescent Health and Development in Context (AHDC) study in Columbus, Ohio._  \n",
    "> OpenNeuro [Dataset] doi:10.18112/openneuro.ds005901.v1.0.0  \n",
    "> https://github.com/OpenNeuroDatasets/ds005901\n",
    "\n",
    "This dataset was collected as part of a **longitudinal neuroimaging study of adolescent health and development**. Participants completed surveys, smartphone-based ecological momentary assessments, and MRI scans across multiple waves. One aim was to understand how **community exposures and reward processes** shape brain function and substance use risk.\n",
    "\n",
    "We'll focus on the **Monetary Incentive Delay (MID)** task, which measures reward anticipation and outcome processing. You can choose to analyze any of the four subjects' data from the first MID run.\n",
    "\n",
    "> **Before you begin, set `SUBJECT = \"sub‑s001\"`, `\"sub‑s002\"`, `\"sub‑s003\"`, or `\"sub‑s004\"`** in the code cell below.\n",
    "\n",
    "Each subject's folder contains:  \n",
    "- a 4D BOLD fMRI volume:  \n",
    "  `sub‑s00X_task-mid_run-01_bold.nii[.gz]`  \n",
    "- its trial timing file:  \n",
    "  `sub‑s00X_task-mid_run-01_events.tsv`  \n",
    "- (you'll also need the bilateral NAcc ROI mask: `nacc_bilateral_mask.nii`, placed in the data folder)\n",
    "\n",
    "**Expected directory structure:**\n",
    "\n",
    "```\n",
    "exercise9/\n",
    "├── Exercise9.ipynb ← This notebook\n",
    "├── requirements.txt ← Pre-bundled dependencies\n",
    "├── Figs/ ← Saved figures will go here\n",
    "└── data/\n",
    "    ├── nacc_bilateral_mask.nii\n",
    "    ├── qc_nacc_roi_alignment.png ← Static QC image\n",
    "    ├── sub‑s001/\n",
    "    │   └── func/\n",
    "    │       ├── sub‑s001_task-mid_run-01_bold.nii[.gz]\n",
    "    │       └── sub‑s001_task-mid_run-01_events.tsv\n",
    "    ├── sub‑s002/\n",
    "    │   └── func/\n",
    "    │       ├── sub‑s002_task-mid_run-01_bold.nii[.gz]\n",
    "    │       └── sub‑s002_task-mid_run-01_events.tsv\n",
    "    ├── sub‑s003/\n",
    "    │   └── func/\n",
    "    │       ├── sub‑s003_task-mid_run-01_bold.nii[.gz]\n",
    "    │       └── sub‑s003_task-mid_run-01_events.tsv\n",
    "    └── sub‑s004/\n",
    "        └── func/\n",
    "            ├── sub‑s004_task-mid_run-01_bold.nii[.gz]\n",
    "            └── sub‑s004_task-mid_run-01_events.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────\n",
    "# 0. Data folder constants\n",
    "# ──────────────────────────────────\n",
    "\n",
    "# Data will be stored in the 'data' subfolder\n",
    "DATA_ROOT = Path(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef57f72",
   "metadata": {},
   "source": [
    "## Select Your Subject\n",
    "\n",
    "Choose which subject's data you'd like to analyze. Available subjects: sub-s001, sub-s002, sub-s003, sub-s004.\n",
    "\n",
    "> **Set `SUBJECT = \"sub‑s001\"`, `\"sub‑s002\"`, `\"sub‑s003\"`, or `\"sub‑s004\"`** in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to \"sub-s001\", \"sub-s002\", \"sub-s003\", or \"sub-s004\" before you start!\n",
    "SUBJECT = \"sub-s003\"\n",
    "\n",
    "# Pre-defined file paths\n",
    "FUNC_DIR = DATA_ROOT / SUBJECT / \"func\"\n",
    "BOLD = FUNC_DIR / f\"{SUBJECT}_task-mid_run-01_bold.nii\"\n",
    "BOLD_GZ = FUNC_DIR / f\"{SUBJECT}_task-mid_run-01_bold.nii.gz\"\n",
    "EVENT = FUNC_DIR / f\"{SUBJECT}_task-mid_run-01_events.tsv\"\n",
    "MASK = DATA_ROOT / \"nacc_bilateral_mask.nii\"\n",
    "\n",
    "print(\"Selected subject:\", SUBJECT)\n",
    "print(\"  BOLD (uncompressed) →\", BOLD)\n",
    "print(\"  BOLD (compressed)   →\", BOLD_GZ)\n",
    "print(\"  EVENTS →\", EVENT)\n",
    "print(\"  MASK   →\", MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef825d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────\n",
    "# Robust downloads from OpenNeuro (with GitHub fallback)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import urllib.request, shutil, gzip, os\n",
    "\n",
    "DATASET = \"ds005901\"\n",
    "SNAPSHOT = \"1.0.0\"  # pin for reproducibility\n",
    "def on_file_url(subject, fname):\n",
    "    # e.g., sub-s002:func:sub-s002_task-mid_run-01_bold.nii.gz\n",
    "    return (f\"https://openneuro.org/crn/datasets/{DATASET}/snapshots/{SNAPSHOT}/files/\"\n",
    "            f\"{subject}:func:{fname}?download=1\")\n",
    "\n",
    "def gh_file_url(subject, fname):\n",
    "    # GitHub fallback (may return git-annex pointer for big files!)\n",
    "    return f\"https://github.com/OpenNeuroDatasets/{DATASET}/raw/main/{subject}/func/{fname}\"\n",
    "\n",
    "def is_gzip_file(p: Path, nbytes=2) -> bool:\n",
    "    try:\n",
    "        with open(p, \"rb\") as f:\n",
    "            return f.read(nbytes) == b\"\\x1f\\x8b\"\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "def download_with_fallback(urls, output_path: Path) -> Path | None:\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # cached?\n",
    "    if output_path.exists():\n",
    "        print(f\"✅ Using cached: {output_path.name}\")\n",
    "        return output_path\n",
    "    # try candidates\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        try:\n",
    "            print(f\"⬇️  [{i}/{len(urls)}] {output_path.name} ← {url}\")\n",
    "            with urllib.request.urlopen(url) as r, open(output_path, \"wb\") as out:\n",
    "                shutil.copyfileobj(r, out)\n",
    "            print(\"   …downloaded\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  failed: {e}\")\n",
    "    print(\"❌ All download sources failed.\")\n",
    "    return None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "# Build filenames and URLs\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "bold_fname   = f\"{SUBJECT}_task-mid_run-01_bold.nii.gz\"\n",
    "events_fname = f\"{SUBJECT}_task-mid_run-01_events.tsv\"\n",
    "\n",
    "bold_urls = [\n",
    "    on_file_url(SUBJECT, bold_fname),     # OpenNeuro primary\n",
    "    gh_file_url(SUBJECT, bold_fname),     # GitHub fallback (may be pointer)\n",
    "]\n",
    "events_urls = [\n",
    "    on_file_url(SUBJECT, events_fname),\n",
    "    gh_file_url(SUBJECT, events_fname),\n",
    "]\n",
    "\n",
    "print(f\"🔍 Checking data files for {SUBJECT}…\")\n",
    "bold_path_gz = FUNC_DIR / bold_fname\n",
    "evt_path     = FUNC_DIR / events_fname\n",
    "\n",
    "bold_gz = download_with_fallback(bold_urls, bold_path_gz)\n",
    "events  = download_with_fallback(events_urls, evt_path)\n",
    "\n",
    "# sanity: confirm BOLD is truly gzip (not a pointer HTML/text)\n",
    "if bold_gz and not is_gzip_file(bold_gz):\n",
    "    print(\"🧪 The downloaded BOLD file is not gzip (likely a git-annex pointer). Deleting it.\")\n",
    "    try: os.remove(bold_gz)\n",
    "    except OSError: pass\n",
    "    bold_gz = None\n",
    "\n",
    "# Use gz directly (nilearn/nibabel handle .nii.gz)\n",
    "BOLD = bold_gz if bold_gz else None\n",
    "EVENT = events\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📍 File paths:\")\n",
    "print(f\"  BOLD:   {BOLD} (exists: {BOLD.exists() if BOLD else False})\")\n",
    "print(f\"  EVENTS: {EVENT} (exists: {EVENT.exists() if EVENT else False})\")\n",
    "print(f\"  MASK:   {MASK} (exists: {MASK.exists()})\")\n",
    "assert BOLD and BOLD.exists(),  \"Missing BOLD file after download.\"\n",
    "assert EVENT and EVENT.exists(), \"Missing events.tsv after download.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39574601",
   "metadata": {},
   "source": [
    "## 1. QC Anatomy & File Sanity Checks\n",
    "\n",
    "Before analyzing brain activity, it's important to confirm that our data are correctly loaded and properly aligned. In the cell below we:\n",
    "\n",
    "1. **Define file paths** to the subject's BOLD fMRI scan, the events timing file, and the NAcc region-of-interest (ROI) mask.  \n",
    "2. **Check** that each file exists on your computer.  \n",
    "3. **Compute** a mean image from the 4D BOLD volume to give a single, easy-to-view anatomical image.  \n",
    "4. **Overlay** the ROI mask (in blue) and a marker for the Ventral Tegmental Area (VTA) (in yellow) to check alignment.\n",
    "\n",
    "**What is the \"mask\"?**  \n",
    "A **mask** in fMRI is a simplified image used to isolate specific brain regions for analysis. It acts like a stencil or cookie-cutter: white areas highlight the voxels to include (e.g., the nucleus accumbens), and black areas hide everything else. By checking the mask placement on the mean image, we ensure we're analyzing the correct brain structure before diving into the data.\n",
    "\n",
    "This quality control step helps prevent misleading results caused by misaligned ROIs or missing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e480eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 1. File existence checks & display static QC image\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"📊 Analyzing: {SUBJECT}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  BOLD   exists? {BOLD.exists()}\")\n",
    "print(f\"  EVENTS exists? {EVENT.exists()}\")\n",
    "print(f\"  MASK   exists? {MASK.exists()}\")\n",
    "\n",
    "# Display pre-made QC image showing NAcc ROI alignment\n",
    "qc_image_path = DATA_ROOT / \"qc_nacc_roi_alignment.png\"\n",
    "if qc_image_path.exists():\n",
    "    print(f\"\\n📍 QC Image: NAcc ROI Alignment Check\")\n",
    "    display(Image(str(qc_image_path)))\n",
    "else:\n",
    "    print(f\"\\n⚠️  QC image not found at: {qc_image_path}\")\n",
    "    print(\"   Expected: Static image showing NAcc mask (blue) overlaid on mean functional image\")\n",
    "    print(\"   Contact instructor for the QC image file.\")\n",
    "\n",
    "# Check if all required files exist before proceeding\n",
    "missing_files = []\n",
    "if not BOLD.exists():\n",
    "    missing_files.append(f\"   - BOLD: {BOLD}\")\n",
    "if not EVENT.exists():\n",
    "    missing_files.append(f\"   - EVENTS: {EVENT}\")\n",
    "if not MASK.exists():\n",
    "    missing_files.append(f\"   - MASK: {MASK}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n❌ Missing required files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "    print(\"\\n⚠️  Please ensure all data files are in place before continuing.\")\n",
    "    print(\"   The download cell above should have fetched BOLD and EVENTS files.\")\n",
    "    print(\"   The MASK file should be provided by your instructor in the data/ folder.\")\n",
    "else:\n",
    "    print(f\"\\n✅ All required files found! Ready to proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96518fe",
   "metadata": {},
   "source": [
    "### Interpreting the QC Image & Checklist\n",
    "\n",
    "**Static QC Image shows:**\n",
    "- **Anatomical Brain Slices:** Three orthogonal (coronal, sagittal, axial) views of the MNI brain template.\n",
    "- **Crosshairs & Coordinates:** White crosshairs intersect at the approximate NAcc centre (MNI: –8, 0, 8).\n",
    "- **Orientation:** “L” and “R” indicate left/right hemispheres in neurological convention.\n",
    "\n",
    "**✅ QC Checklist - Verify before proceeding:**\n",
    "1. All three files (BOLD, EVENT, MASK) exist and are accessible.  \n",
    "2. The crosshairs sit over the ventral striatum region (just below the anterior commissure).  \n",
    "3. The intersection point matches both left and right NAcc locations.  \n",
    "4. The displayed coordinates make anatomical sense relative to known brain landmarks.  \n",
    "\n",
    "> **Why this QC step matters:**  \n",
    "> By confirming file accessibility and visually checking that the crosshairs are positioned in the ventral striatum, you ensure that the time-series you extract in the next step actually comes from the intended anatomical region. If the alignment or coordinates are off, all downstream comparisons (Reward vs Neutral) could be invalid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d45e1",
   "metadata": {},
   "source": [
    "## 2. Extract NAcc Time-Series\n",
    "\n",
    "Before we dive in, let's simplify **TR** (Repetition Time):\n",
    "\n",
    "> **TR = the interval between \"snapshots.\"**  \n",
    "> Every TR seconds (e.g. 2 s) the scanner takes one full 3D \"photo\" of your brain. When we want the BOLD signal right after an event, we convert the event time (in seconds) into which snapshot number (TR index) to pull.\n",
    "\n",
    "In the cell below we use a helper function `extract_psc()` that:\n",
    "\n",
    "1. **Loads** the 4D BOLD time-series image\n",
    "2. **Resamples the ROI mask** to match the BOLD data's voxel grid and coordinate system  \n",
    "   🧠 This ensures each mask voxel correctly overlaps with corresponding BOLD voxels\n",
    "3. **Applies the mask** to extract BOLD values from NAcc voxels across all time points  \n",
    "   🎯 Isolates the signal over time from just the NAcc—ignoring everything else in the brain\n",
    "4. **Averages across voxels** at each TR to produce a single NAcc time-series  \n",
    "5. **Converts to percent-signal-change (PSC)** for standardized, interpretable units\n",
    "\n",
    "The function returns both:\n",
    "- `nacc_psc`: percent-signal-change time-series (for analysis)\n",
    "- `nacc_ts`: raw signal time-series (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a223100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 2. Extract NAcc time-series\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def extract_psc(ts_img, mask_img):\n",
    "    \"\"\"\n",
    "    Helper function to extract percent-signal-change time-series from an ROI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_img : str or Nifti-like\n",
    "        Path to 4D BOLD time-series image\n",
    "    mask_img : str or Nifti-like  \n",
    "        Path to binary ROI mask\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    psc_ts : numpy.ndarray\n",
    "        1D array of percent-signal-change values (one per TR)\n",
    "    raw_ts : numpy.ndarray\n",
    "        1D array of raw signal values (one per TR)\n",
    "    \"\"\"\n",
    "    # Load the 4D BOLD image\n",
    "    bold_img = image.load_img(str(ts_img))\n",
    "    \n",
    "    # Resample mask to BOLD space (nearest-neighbor keeps it binary)\n",
    "    mask_res = resample_to_img(\n",
    "        source_img=str(mask_img),\n",
    "        target_img=bold_img,\n",
    "        interpolation=\"nearest\"\n",
    "    )\n",
    "    \n",
    "    # Extract time-series from all voxels in the mask\n",
    "    # apply_mask returns array of shape (n_volumes, n_voxels)\n",
    "    roi_vox_ts = apply_mask(bold_img, mask_res)\n",
    "    \n",
    "    # Average across voxels to get single ROI trace per TR\n",
    "    # result is 1D array of length n_volumes\n",
    "    raw_ts = roi_vox_ts.mean(axis=1)\n",
    "    \n",
    "    # Convert to percent-signal-change (PSC)\n",
    "    baseline = raw_ts.mean()\n",
    "    psc_ts = 100 * (raw_ts - baseline) / baseline\n",
    "    \n",
    "    print(f\"Extracted time-series length: {len(psc_ts)} TRs\")\n",
    "    print(f\"Baseline mean signal = {baseline:.1f}; converted to PSC.\")\n",
    "    \n",
    "    return psc_ts, raw_ts\n",
    "\n",
    "# 2a) Extract NAcc time-series using our helper function\n",
    "nacc_psc, nacc_ts = extract_psc(BOLD, MASK)\n",
    "\n",
    "# 2b) Quick peek: show first k rows of the PSC time-series in a table\n",
    "df_nacc = pd.DataFrame(nacc_psc, columns=[\"psc_nacc\"])\n",
    "\n",
    "# interactive slider to choose how many rows to display\n",
    "interact(\n",
    "    lambda k: df_nacc.head(k),\n",
    "    k=(1, min(20, len(df_nacc)), 1)  # slider from 1 up to 20 (or total TRs if fewer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576f363",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "- **Raw → PSC:** We calculated the average NAcc signal across the entire run (`baseline`), then converted each TR’s mean activation into **percent-signal-change** (`nacc_psc`). Now “0 %” means no change from baseline, and “+1 %” means a 1 % increase.\n",
    "- **What is PSC?** **Percent-Signal-Change (PSC)** expresses each TR’s BOLD signal as a percentage change from its baseline, effectively measuring the percent change in blood flow (and thus neural activity) within your ROI (Brain Region of Interest).\n",
    "- **Interactive preview:** The slider lets you inspect the first k rows of the PSC time-series. Try moving it between 1 and 20 (or more) to see how NAcc activation evolves over those snapshots.\n",
    "- **Why PSC?** Converting to percent-signal-change puts all activation values on an intuitive, comparable scale—standard practice in fMRI—so your subsequent plots speak in “percent change” rather than arbitrary intensities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab73a3",
   "metadata": {},
   "source": [
    "## 3. Build Reward vs Neutral Arrays\n",
    "\n",
    "In this step we:\n",
    "\n",
    "- **Load** the MID events file and read each trial's onset time (in seconds) and `trial_type`.\n",
    "- **Convert** each onset time to a TR index, then add a **4–5 TR lag** to capture the hemodynamic peak.\n",
    "- **Average** the BOLD signal over the following **2 TRs** to get one activation value per trial.\n",
    "- **Restrict to cue epochs** by filtering rows that start with `Cue_` — this isolates the **anticipation** period (aligns with ABCD MID ARvN betas).\n",
    "- **Classify** `Cue_*Reward*` trials into `reward_vals` and `Cue_Triangle` trials into `neutral_vals`.\n",
    "\n",
    "At the end you'll have two lists (`reward_vals` and `neutral_vals`) ready for plotting and probabilistic comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# 3. Build Reward vs Neutral arrays (using PSC time-series)\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# 3a) Load events.tsv\n",
    "events = pd.read_csv(str(EVENT), sep='\\t')\n",
    "print(events[['onset', 'trial_type']].head())\n",
    "\n",
    "# 3b) Get TR from header\n",
    "TR = image.load_img(str(BOLD)).header.get_zooms()[3]\n",
    "print(f\"Repetition time (TR) = {TR} s\")\n",
    "\n",
    "# 3c) Define a helper to sample NAcc PSC after onset\n",
    "def get_activation(ts, onset, lag_TRs=5, win_TRs=2, TR=1.0):\n",
    "    \"\"\"\n",
    "    Return mean NAcc percent-signal-change in the window \n",
    "    [onset+lag, onset+lag+win) measured in TRs.\n",
    "    \"\"\"\n",
    "    onset_TR = int(onset / TR)\n",
    "    start = onset_TR + lag_TRs\n",
    "    end = start + win_TRs\n",
    "    return ts[start:end].mean()\n",
    "\n",
    "# 3d) Restrict to cue epochs (anticipation). Build reward vs neutral lists\n",
    "reward_vals = []\n",
    "neutral_vals = []\n",
    "\n",
    "cue_events = events[events['trial_type'].str.startswith('Cue')]\n",
    "\n",
    "for _, trial in cue_events.iterrows():\n",
    "    act = get_activation(\n",
    "        nacc_psc,               # using percent‐signal‐change series\n",
    "        trial['onset'],\n",
    "        lag_TRs=5,\n",
    "        win_TRs=2,\n",
    "        TR=TR\n",
    "    )\n",
    "    if 'Reward' in trial['trial_type']:\n",
    "        reward_vals.append(act)\n",
    "    elif 'Triangle' in trial['trial_type']:\n",
    "        neutral_vals.append(act)\n",
    "\n",
    "# 3e) Quick sanity check\n",
    "print(f\"→ {len(reward_vals)} reward CUE trials\")\n",
    "print(f\"→ {len(neutral_vals)} neutral (Triangle) CUE trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78569be7",
   "metadata": {},
   "source": [
    "**What this output tells us**:\n",
    "\n",
    "- The first five rows confirm we’ve correctly loaded the event file, showing each trial’s onset time (in seconds) and its type.\n",
    "- The **Repetition time (TR) = 1.0 s** means the scanner collected one whole‐brain volume every second.\n",
    "- We found **80 “Reward” trials** and **40 “Triangle” (neutral) trials**.  \n",
    "  These counts become the lengths of our two lists—`reward_vals` (80 samples) and `neutral_vals` (40 samples)—which we’ll compare in the next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e50b0e",
   "metadata": {},
   "source": [
    "### Quick Check: Compute Percent-Signal-Change Means\n",
    "\n",
    "Now that we’ve converted our time-series to percent-signal-change (`nacc_psc`), let’s compute the average PSC for each condition.\n",
    "\n",
    "🧠 **First try writing the code without AI (eg, GitHub Copilot)**—this helps you internalize the syntax.  \n",
    "🤖 **If you get stuck**, let Copilot suggest a hint or fill in the blanks.\n",
    "\n",
    "Below, fill in the two lines to calculate:\n",
    "\n",
    "- `mean_reward`: the average PSC of all NAcc values on **Reward** trials  \n",
    "- `mean_neutral`: the average PSC of all NAcc values on **Neutral (Triangle)** trials  \n",
    "\n",
    "Finally, print their difference (`mean_reward - mean_neutral`) to see how much higher (or lower) the Reward response is compared to Neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Student TO-DO: fill in the two lines below ---\n",
    "mean_reward  = ...   # <- your code here\n",
    "mean_neutral = ...   # <- your code here\n",
    "\n",
    "print(\"Δ mean (Reward – Neutral) =\", mean_reward - mean_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d20d7",
   "metadata": {},
   "source": [
    "## 4. Visualisations\n",
    "\n",
    "Below are the key plots you will generate.  Each cell saves its figure into `figs/` so you can easily pull them into your write‑up. The **Figs** folder is in the same folder as this file alongside our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fcfb6c",
   "metadata": {},
   "source": [
    "### 4a) Event-Locked NAcc Time Course\n",
    "\n",
    "Before comparing trial-averaged activations, let's visualize **when** the NAcc responds to cues by plotting the full time course around cue onset.\n",
    "\n",
    "**What this plot shows:**\n",
    "- **X-axis:** Time relative to cue presentation (seconds). Zero marks the moment the cue appears.\n",
    "- **Y-axis:** NAcc percent-signal-change (PSC), averaged across all trials of each type.\n",
    "- **Shaded bands:** Standard error across trials—wider bands indicate more variability between individual trials.\n",
    "\n",
    "**Why this matters:**\n",
    "- The **hemodynamic response** peaks ~5–7 seconds after a stimulus due to blood flow delays.\n",
    "- By extracting a **peri-stimulus window** (2 TRs before to 14 TRs after cue onset), we capture the full rise, peak, and return-to-baseline of the BOLD signal.\n",
    "- If the Reward curve (orange) rises higher than Neutral (blue) around 5–7 seconds post-cue, it confirms that **anticipation**—not just outcome—drives NAcc activation.\n",
    "\n",
    "This temporal view complements the box plot (which collapses time) by showing the **dynamics** of reward processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfabae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Build peri-stimulus time courses for Reward vs Neutral cues\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "win_pre, win_post = 2, 14   # TRs before/after cue\n",
    "t = np.arange(-win_pre, win_post) * TR\n",
    "\n",
    "\n",
    "def extract_epoch(ts, onset_s, TR, pre, post):\n",
    "    onset_TR = int(onset_s / TR)\n",
    "    idx = np.arange(onset_TR - pre, onset_TR + post)\n",
    "    valid = (idx >= 0) & (idx < len(ts))\n",
    "    out = np.full(idx.shape, np.nan, dtype=float)\n",
    "    out[valid] = ts[idx[valid]]\n",
    "    return out\n",
    "\n",
    "cue_reward_onsets = cue_events[cue_events['trial_type'].str.contains('Reward')]['onset'].values\n",
    "cue_neutral_onsets = cue_events[cue_events['trial_type'].str.contains('Triangle')]['onset'].values\n",
    "\n",
    "epochs_reward = np.vstack([extract_epoch(nacc_psc, o, TR, win_pre, win_post) for o in cue_reward_onsets])\n",
    "epochs_neutral = np.vstack([extract_epoch(nacc_psc, o, TR, win_pre, win_post) for o in cue_neutral_onsets])\n",
    "\n",
    "m_reward = np.nanmean(epochs_reward, axis=0)\n",
    "m_neutral = np.nanmean(epochs_neutral, axis=0)\n",
    "se_reward = np.nanstd(epochs_reward, axis=0) / np.sqrt(np.sum(~np.isnan(epochs_reward), axis=0))\n",
    "se_neutral = np.nanstd(epochs_neutral, axis=0) / np.sqrt(np.sum(~np.isnan(epochs_neutral), axis=0))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(t, m_neutral, color=\"C0\", label=\"Neutral cue\")\n",
    "plt.fill_between(t, m_neutral - se_neutral, m_neutral + se_neutral, color=\"C0\", alpha=0.2)\n",
    "plt.plot(t, m_reward, color=\"C1\", label=\"Reward cue\")\n",
    "plt.fill_between(t, m_reward - se_reward, m_reward + se_reward, color=\"C1\", alpha=0.2)\n",
    "plt.axvline(0, color=\"k\", linestyle=\":\", linewidth=1)\n",
    "plt.xlabel(\"Time from cue (s)\")\n",
    "plt.ylabel(\"NAcc PSC (%)\")\n",
    "plt.title(\"Event-locked NAcc PSC around cue onset\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/anticipation_timecourse.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44cdb7",
   "metadata": {},
   "source": [
    "### Interpreting the Event-Locked Timecourse\n",
    "\n",
    "**What to look for:**\n",
    "1. **Baseline period (−2 to 0 s):** Both curves should hover near zero PSC before the cue appears—confirming our PSC baseline is valid.\n",
    "2. **Rise phase (0 to ~5 s):** Both curves climb as blood flows to the NAcc in response to the cue, but does one rise faster or higher?\n",
    "3. **Peak (~5–7 s):** The maximum PSC typically occurs here due to the hemodynamic lag. Look for separation between Reward and Neutral traces.\n",
    "4. **Return to baseline (>10 s):** Both curves should descend back toward zero as the anticipation period ends.\n",
    "\n",
    "**Key observations:**\n",
    "- **Reward > Neutral during anticipation:** If the orange line stays consistently above blue during the 4–8 second window, it confirms that reward cues elicit **stronger anticipatory activation** in the NAcc—the neural signature of incentive salience.\n",
    "- **Standard error bands:** Overlapping bands indicate high trial-to-trial variability, common in single-subject analyses. At the group level (many subjects), these bands typically separate more clearly.\n",
    "- **Timing validates our lag_TRs choice:** The 5-TR lag we used for `get_activation()` samples the peak of this response curve, ensuring we capture maximum anticipation-related activity.\n",
    "\n",
    "This plot bridges the gap between raw time-series (Section 2) and summary statistics (next sections), showing that the NAcc \"wakes up\" specifically when rewards are anticipated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a387a9",
   "metadata": {},
   "source": [
    "### Box Plot with Mean Lines\n",
    "\n",
    "Here we’ll draw a box plot of Reward vs Neutral activations and overlay horizontal lines at the condition means you computed above (`mean_reward`, `mean_neutral`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Box plot with mean lines\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# 4a) boxplot\n",
    "sns.boxplot(data=[neutral_vals, reward_vals],\n",
    "            palette=[\"C0\",\"C1\"],\n",
    "            showfliers=False)\n",
    "\n",
    "# overlay the means\n",
    "plt.hlines(mean_neutral, -0.4, 0.4, color=\"C0\", linestyle=\"--\",\n",
    "           label=f\"Neutral mean ({mean_neutral:.2f}%)\")\n",
    "plt.hlines(mean_reward,  0.6, 1.4,  color=\"C1\", linestyle=\"--\",\n",
    "           label=f\"Reward mean ({mean_reward:.2f}%)\")\n",
    "\n",
    "plt.xticks([0,1], [\"Neutral cue\",\"Reward cue\"])\n",
    "plt.ylabel(\"NAcc percent signal change (%)\")\n",
    "plt.title(\"4a) NAcc PSC by cue type (box + means)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.savefig(\"figs/4a_box_means.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38bcb2",
   "metadata": {},
   "source": [
    "**Interpretation.**  \n",
    "- The box represents the interquartile range (IQR) of activations; whiskers extend to ±1.5×IQR.  \n",
    "- Dashed lines mark your previously computed means—now it’s clear how far Reward’s mean sits above Neutral’s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5825a",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Interval & Effect Size\n",
    "\n",
    "Now let's quantify the **uncertainty** and **magnitude** of the Reward vs Neutral difference using two complementary statistics:\n",
    "\n",
    "**What this analysis does:**\n",
    "1. **Bootstrap resampling (5,000 iterations):** Randomly samples with replacement from your reward and neutral trial data to simulate what would happen if you ran this experiment thousands of times. This generates a distribution of possible mean differences.\n",
    "2. **95% Confidence Interval (CI):** The range within which we're 95% confident the true population mean difference falls. If this interval excludes zero, the effect is statistically significant.\n",
    "3. **Cohen's d:** A standardized effect size that measures the difference in means relative to the pooled standard deviation. This tells us whether the difference is practically meaningful, not just statistically detectable.\n",
    "\n",
    "**Why both metrics matter:**\n",
    "- **CI answers:** \"How certain are we about the direction and magnitude of the effect?\"\n",
    "- **Cohen's d answers:** \"How large is this effect in real-world terms?\"\n",
    "\n",
    "**Interpreting the output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════\n",
    "# Bootstrap CI and Cohen's d effect size summary\n",
    "# ════════════════════════════════════════════════════════════════════\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "B = 5000\n",
    "boot_diff = []\n",
    "for _ in range(B):\n",
    "    r = rng.choice(reward_vals, size=len(reward_vals), replace=True)\n",
    "    n = rng.choice(neutral_vals, size=len(neutral_vals), replace=True)\n",
    "    boot_diff.append(r.mean() - n.mean())\n",
    "ci_lo, ci_hi = np.percentile(boot_diff, [2.5, 97.5])\n",
    "\n",
    "def cohens_d(a, b):\n",
    "    a, b = np.asarray(a), np.asarray(b)\n",
    "    s1, s2 = a.std(ddof=1), b.std(ddof=1)\n",
    "    sp = np.sqrt(((len(a)-1)*s1**2 + (len(b)-1)*s2**2) / (len(a)+len(b)-2))\n",
    "    return (a.mean() - b.mean()) / sp\n",
    "\n",
    "d = cohens_d(reward_vals, neutral_vals)\n",
    "print(f\"Δ mean = {mean_reward - mean_neutral:.3f}%  (95% CI [{ci_lo:.3f}, {ci_hi:.3f}]),  Cohen's d = {d:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f253fc",
   "metadata": {},
   "source": [
    "### Interpreting Bootstrap CI & Cohen's d\n",
    "\n",
    "**Understanding the printed statistics:**\n",
    "\n",
    "| Statistic | Interpretation |\n",
    "|-----------|----------------|\n",
    "| **Δ mean = X.XX%** | Average difference in NAcc PSC between Reward and Neutral cues across all trials. Positive = Reward higher. |\n",
    "| **95% CI [lo, hi]** | We're 95% confident the true mean difference lies in this range. If both bounds are positive (> 0), Reward significantly exceeds Neutral. |\n",
    "| **Cohen's d** | Standardized effect size:<br>• **d < 0.2:** Trivial effect<br>• **0.2–0.5:** Small effect<br>• **0.5–0.8:** Medium effect<br>• **d > 0.8:** Large effect |\n",
    "\n",
    "**What this tells us:**\n",
    "- **If CI excludes zero (e.g., [0.05, 0.31]):** The reward anticipation effect is **statistically reliable**—even accounting for trial-to-trial noise, we'd expect Reward cues to consistently activate NAcc more than Neutral cues.\n",
    "- **If d ≈ 0.3–0.5:** The effect is **small to medium** in magnitude—detectable, but subject to individual differences and measurement noise (typical for single-subject fMRI).\n",
    "- **If d > 0.5:** The effect is **moderate to large**—reward cues drive a substantial difference in NAcc activation, supporting the incentive-salience hypothesis even at the individual level.\n",
    "\n",
    "**Connecting to neuroscience:**\n",
    "This quantitative summary bridges the visual patterns (timecourse, box plot) and the underlying biology: a Cohen's d of 0.4–0.6 suggests that dopamine-driven anticipation produces a **moderate but consistent shift** in NAcc activity—exactly what we'd predict if the brain is learning to \"want\" reward-predicting cues more over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0d29f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5 · Key Take-Aways    \n",
    "\n",
    "### What did we see in the MID task?\n",
    "\n",
    "**Main finding:** The NAcc showed **higher activation for Reward vs Neutral cues** during the anticipation window.\n",
    "\n",
    "| Evidence | What it means |\n",
    "|----------|---------------|\n",
    "| **Event-locked timecourse** | Reward cues (orange) peaked higher than Neutral cues (blue) ~5–7 seconds post-cue—exactly when anticipation is strongest. |\n",
    "| **Box plot + means** | Reward trials showed consistently higher PSC, with the mean difference captured by the dashed lines. |\n",
    "| **Bootstrap CI & Cohen's d** | The 95% CI excluded zero (statistically reliable effect), and Cohen's d quantified a small-to-medium effect size typical of single-subject fMRI. |\n",
    "\n",
    "**In plain language:** When the \"win-money\" cue appeared, the nucleus accumbens—the brain's \"want-it\" hub—showed a larger percent-signal-change than when the neutral cue appeared. This is the neural signature of **incentive salience**.\n",
    "\n",
    "---\n",
    "\n",
    "### How does this link back to *BuildingABrain.ipynb*?\n",
    "\n",
    "Both your toy neural network and the human brain use the same three-step loop to learn from rewards:\n",
    "\n",
    "| Step | Artificial Network | Human Brain |\n",
    "|------|-------------------|-------------|\n",
    "| **1. Teaching signal** | Computes prediction error after each guess → updates weights | VTA dopamine burst when outcome > expectation → modulates synapses |\n",
    "| **2. Value learning** | Weights strengthen toward rewarded choices across training epochs | Cortico-striatal synapses strengthen with repeated reward → NAcc \"wants\" cue more |\n",
    "| **3. Behavioral output** | Network selects higher-valued class at test time | Elevated NAcc activity drives faster responses when money is at stake |\n",
    "\n",
    "---\n",
    "\n",
    "### Why does it matter?\n",
    "\n",
    "**Mechanistic insight:**  \n",
    "Seeing NAcc PSC rise after reward cues provides a concrete, measurable readout of the abstract \"prediction-error learning\" you coded in BuildingABrain. The same computational principles—**error signals**, **weight updates**, **learned associations**—operate in both silicon and neurons.\n",
    "\n",
    "**Clinical relevance:**  \n",
    "If dopamine bursts are hijacked (e.g., by addictive drugs or maladaptive cues), this same learning loop can over-train the NAcc, inflating \"wanting\" for harmful stimuli. Understanding the mechanism is the first step toward interventions that re-wire these associations.\n",
    "\n",
    "---\n",
    "\n",
    "**Take a moment to look back at your three plots—each one is a puzzle piece showing *when* (timecourse), *how much* (box plot), and *how certain* (bootstrap CI) the NAcc responds to anticipated rewards.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
